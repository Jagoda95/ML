{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Kopia notatnika 12_RNN.ipynb","provenance":[{"file_id":"https://github.com/gmum/ml2020-21/blob/master/lab/11_RNN.ipynb","timestamp":1610655233515},{"file_id":"13ODRWR6EtqQ8hqEWUX41fMt22yiy8jRs","timestamp":1610372422692}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"P95c6hK3hAQq"},"source":["# Rekurencyjne Sieci Neuronowe (RNN)"]},{"cell_type":"markdown","metadata":{"id":"laVdd5g5hAQu"},"source":["### Importy i Utilsy  (odpalić i schować )"]},{"cell_type":"code","metadata":{"id":"I0D3yk7lhAQu","executionInfo":{"status":"ok","timestamp":1611176059281,"user_tz":-60,"elapsed":661,"user":{"displayName":"Jagoda Dziadosz","photoUrl":"","userId":"11816835418749834277"}}},"source":["# imports \n","import torch\n","import os\n","import unicodedata\n","import string\n","import numpy as np\n","from typing import Tuple, Optional, List\n","\n","from torch.nn.functional import cross_entropy\n","\n","import matplotlib.pyplot as plt \n","from sklearn.metrics import f1_score\n","\n","from torch.utils.data import Dataset, DataLoader\n","\n","all_letters = string.ascii_letters\n","n_letters = len(all_letters)\n","\n","\n","class ListDataset(Dataset):\n","    \n","    def __init__(self, data, targets):\n","        \n","        self.data = data\n","        self.targets = targets\n","        \n","    def __getitem__(self, ind):\n","        \n","        return self.data[ind], self.targets[ind]\n","    \n","    def __len__(self):\n","        return len(self.targets)\n","\n","    \n","def unicode_to__ascii(s: str) -> str:\n","    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn'\n","                                                                 and c in all_letters)\n","                   \n","\n","def read_lines(filename: str) -> List[str]:\n","    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n","    return [unicode_to__ascii(line) for line in lines]\n","\n","\n","def letter_to_index(letter: str) -> int:\n","    return all_letters.find(letter)\n","\n","\n","def line_to_tensor(line: str) -> torch.Tensor:\n","    tensor = torch.zeros(len(line), n_letters)\n","    for i, letter in enumerate(line):\n","        tensor[i][letter_to_index(letter)] = 1\n","    return tensor"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RcSQvaMPhAQv"},"source":["## Dane sekwencyjne\n","\n","Modele, którymi zajmowaliśmy się wcześniej zakładały konkretny kształt danych. Dla przykładu klasyczna sieć neuronowa fully-connected dla MNISTa zakładała, że na wejściu dostanie wektory rozmiaru 784 - dla wektorów o innej wymiarowości i innych obiektów model zwyczajnie nie będzie działać.\n","\n","Takie założenie bywa szczególnie niewygodne przy pracy z niektórymi typami danych, takimi jak:\n","* językiem naturalny (słowa czy zdania mają zadanej z góry liczby znaków)\n","* szeregi czasowe (dane giełdowe ciągną się właściwie w nieskończoność) \n","* dźwięk (nagrania mogą być krótsze lub dłuższe).\n","\n","Do rozwiązania tego problemu służą rekuencyjne sieci neuronowe (*recurrent neural networks, RNNs*), które zapamiętują swój stan z poprzedniej iteracji."]},{"cell_type":"markdown","metadata":{"id":"mH3chO87hAQv"},"source":["### Ładowanie danych\n","Poniższe dwie komórki ściągają dataset nazwisk z 18 różnych narodowości. Każda litera w danym nazwisku jest zamieniana na jej indeks z alfabetu w postaci kodowania \"one-hot\". Inaczej mówiąc, każde nazwisko jest binarną macierzą rozmiaru `len(name)` $\\times$ `n_letters`. \n","\n","Dodatkowo, ponieważ ten dataset jest mocno niezbalansowany, użyjemy specjalnego samplera do losowania przykładów treningowych, tak aby do uczenia sieć widziała tyle samo przykładów z każdej klasy.\n","\n","Ponieważ nazwiska mogą mieć różne długości będziemy rozważać `batch_size = 1` w tym notebooku (choć implementacje modeli powinny działać dla dowolnych wartości `batch_size`!)"]},{"cell_type":"code","metadata":{"id":"maOHB6NZiRgr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611176068781,"user_tz":-60,"elapsed":1140,"user":{"displayName":"Jagoda Dziadosz","photoUrl":"","userId":"11816835418749834277"}},"outputId":"b0ec7a7e-b603-4ae2-f94b-f377468cde5f"},"source":["!wget https://download.pytorch.org/tutorial/data.zip\n","!unzip data.zip"],"execution_count":3,"outputs":[{"output_type":"stream","text":["--2021-01-20 20:54:27--  https://download.pytorch.org/tutorial/data.zip\n","Resolving download.pytorch.org (download.pytorch.org)... 13.32.204.49, 13.32.204.93, 13.32.204.65, ...\n","Connecting to download.pytorch.org (download.pytorch.org)|13.32.204.49|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2882130 (2.7M) [application/zip]\n","Saving to: ‘data.zip’\n","\n","\rdata.zip              0%[                    ]       0  --.-KB/s               \rdata.zip            100%[===================>]   2.75M  --.-KB/s    in 0.05s   \n","\n","2021-01-20 20:54:27 (60.8 MB/s) - ‘data.zip’ saved [2882130/2882130]\n","\n","Archive:  data.zip\n","   creating: data/\n","  inflating: data/eng-fra.txt        \n","   creating: data/names/\n","  inflating: data/names/Arabic.txt   \n","  inflating: data/names/Chinese.txt  \n","  inflating: data/names/Czech.txt    \n","  inflating: data/names/Dutch.txt    \n","  inflating: data/names/English.txt  \n","  inflating: data/names/French.txt   \n","  inflating: data/names/German.txt   \n","  inflating: data/names/Greek.txt    \n","  inflating: data/names/Irish.txt    \n","  inflating: data/names/Italian.txt  \n","  inflating: data/names/Japanese.txt  \n","  inflating: data/names/Korean.txt   \n","  inflating: data/names/Polish.txt   \n","  inflating: data/names/Portuguese.txt  \n","  inflating: data/names/Russian.txt  \n","  inflating: data/names/Scottish.txt  \n","  inflating: data/names/Spanish.txt  \n","  inflating: data/names/Vietnamese.txt  \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DRGjkPZ2hAQv","executionInfo":{"status":"ok","timestamp":1611176072149,"user_tz":-60,"elapsed":2198,"user":{"displayName":"Jagoda Dziadosz","photoUrl":"","userId":"11816835418749834277"}}},"source":["# NOTE: you can change the seed or remove it completely if you like\n","torch.manual_seed(1337)\n","\n","data_dir = 'data/names'\n","\n","data = []\n","targets = [] \n","label_to_idx = {}\n","\n","# read each natonality file and process data \n","for label, file_name in enumerate(os.listdir(data_dir)):\n","    \n","    label_to_idx[label] = file_name.split('.')[0].lower() #{0: 'czech', 1: 'greek', ... 16: 'polish', 17: 'chinese'}\n","    \n","    names = read_lines(os.path.join(data_dir, file_name)) #[Lin, Lee, ... Yun, Ang]\n","    data += [line_to_tensor(name) for name in names] # zero_tensors\n","    targets += len(names) * [label] # [0,0,0...,3,3,3,...18,18,18]\n","\n","# split into train and test indices\n","test_frac = 0.1\n","n_test = int(test_frac * len(targets)) # 2007\n","test_ind = np.random.choice(len(targets), size=n_test, replace=False) #[14747    25  9866 ...  9582   308  3339]\n","train_ind = np.setdiff1d(np.arange(len(targets)), test_ind) #[    0     1     3 ... 20071 20072 20073]\n","\n","# print(targets)\n","\n","\n","targets = torch.tensor(targets) # targets tylko, że w tensor XD\n","train_targets = targets[train_ind] #tensor([ 0,  0,  0,  ..., 17, 17, 17]) len = 18067\n","\n","# calculate weights for BalancedSampler\n","uni, counts = np.unique(train_targets, return_counts=True)\n","weight_per_class = len(targets) / counts\n","weight = [weight_per_class[c] for c in train_targets]\n","# preapre the sampler\n","sampler = torch.utils.data.sampler.WeightedRandomSampler(weights=weight, num_samples=len(weight)) #<torch.utils.data.sampler.WeightedRandomSampler object at 0x7f7fd8c68c18> type = torch.utils.data.sampler.WeightedRandomSampler\n","\n","train_dataset = ListDataset(data=[x for i, x in enumerate(data) if i in train_ind], targets=train_targets)\n","train_loader = DataLoader(train_dataset, shuffle=False, batch_size=1, sampler=sampler)\n","\n","test_dataset = ListDataset(data=[x for i, x in enumerate(data) if i in test_ind], targets=targets[test_ind])\n","test_loader = DataLoader(test_dataset, shuffle=False, batch_size=1)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"wPgnAO-xx5yT","executionInfo":{"status":"ok","timestamp":1611176073569,"user_tz":-60,"elapsed":608,"user":{"displayName":"Jagoda Dziadosz","photoUrl":"","userId":"11816835418749834277"}}},"source":["# print(data[0])\r\n","# print(targets)\r\n","# print(label_to_idx)\r\n","# print(names)\r\n","# print(n_test)\r\n","# print(test_ind)\r\n","# print(train_ind)\r\n","# print(train_targets)\r\n","# print(sampler)\r\n","# print(train_dataset)\r\n","# print(train_loader)\r\n","# print(test_dataset)\r\n","# print(test_loader)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yvstu1-sldC6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611176074808,"user_tz":-60,"elapsed":549,"user":{"displayName":"Jagoda Dziadosz","photoUrl":"","userId":"11816835418749834277"}},"outputId":"f83fb507-cf57-40f9-a9f0-33ef01e0ddab"},"source":["# check out the content of the dataset\n","for i, (x, y) in enumerate(train_loader):\n","    break\n","\n","print(\"x.shape:\", x.shape)\n","print(\"name: \", end=\"\")\n","for letter_onehot in x[0]:\n","    print(all_letters[torch.argmax(letter_onehot)], end=\"\")\n","\n","print(\"\\ny:\", label_to_idx[y.item()])\n","# print(\"\\ny:\", y.item())\n","# print(y)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["x.shape: torch.Size([1, 4, 52])\n","name: Rios\n","y: portuguese\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"x3VdtPOhhAQw"},"source":["## Zadanie 1. (2 pkt.)\n","\n","Zaimplementuj \"zwykłą\" sieć rekurencyjną. \n","![rnn](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)\n","\n","* W klasie `RNN` należy zainicjalizować potrzebne wagi oraz zaimplementować główną logikę dla pojedynczej chwili czasowej $x_t$\n","* Wyjście z sieci możemy mieć dowolny rozmiar, potrzebna jest również warstwa przekształacjąca stan ukryty na wyjście.\n","* W pętli uczenia należy dodać odpowiednie wywołanie sieci. HINT: pamiętać o iterowaniu po wymiarze \"czasowym\".\n","* Zalecane jest użycie aktywacji na warstwie liczącej reprezentacje `hidden` tak, aby nie \"eksplodowała\", np. `tanh`.\n"]},{"cell_type":"code","metadata":{"id":"WNu0vccJhAQw","executionInfo":{"status":"ok","timestamp":1611176915237,"user_tz":-60,"elapsed":1721,"user":{"displayName":"Jagoda Dziadosz","photoUrl":"","userId":"11816835418749834277"}}},"source":["import matplotlib.pyplot as plt\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class RNN(torch.nn.Module):\n","    \n","    def __init__(self, \n","                 input_size: int,\n","                 hidden_size: int, \n","                 output_size: int):\n","        \"\"\"\n","        :param input_size: int\n","            Dimensionality of the input vector\n","        :param hidden_size: int\n","            Dimensionality of the hidden space\n","        :param output_size: int\n","            Desired dimensionality of the output vector\n","        \"\"\"\n","        super(RNN, self).__init__()\n","        self.input_size = input_size\n","\n","        self.hidden_size = hidden_size\n","\n","        self.input_to_hidden = nn.Linear(input_size+hidden_size,hidden_size)\n","        \n","        self.hidden_to_output = nn.Linear(hidden_size,output_size)\n","    \n","    # for the sake of simplicity a single forward will process only a single timestamp \n","    def forward(self, \n","                input: torch.tensor, \n","                hidden: torch.tensor) -> Tuple[torch.tensor, torch.tensor]:\n","        \"\"\"\n","        :param input: torch.tensor \n","            Input tesnor for a single observation at timestep t\n","            shape [batch_size, input_size]\n","        :param hidden: torch.tensor\n","            Representation of the memory of the RNN from previous timestep\n","            shape [batch_size, hidden_size]\n","        \"\"\"\n","        \n","        combined = torch.cat([input, hidden], dim=1) \n","        \n","        hidden = torch.tanh(self.input_to_hidden(combined))\n","        output =  self.hidden_to_output(hidden)\n","        return output, hidden\n","    \n","    def init_hidden(self, batch_size: int) -> torch.Tensor:\n","        \"\"\"\n","        Returns initial value for the hidden state\n","        \"\"\"\n","        return torch.zeros(batch_size, self.hidden_size, requires_grad=True).cuda()"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LIe3L-8LhAQw"},"source":["### Pętla uczenia"]},{"cell_type":"code","metadata":{"id":"xXEsqqvxhAQx","scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611177040126,"user_tz":-60,"elapsed":46947,"user":{"displayName":"Jagoda Dziadosz","photoUrl":"","userId":"11816835418749834277"}},"outputId":"7b4c97b6-0bba-4915-df00-2945b4de74f4"},"source":["n_class = len(label_to_idx)\n","\n","# initialize network and optimizer\n","rnn = RNN(n_letters, 256, n_class).cuda()\n","optimizer = torch.optim.SGD(rnn.parameters(), lr=0.01)   \n","\n","# we will train for only a single epoch \n","epochs = 1\n","\n","\n","# main loop\n","for epoch in range(epochs):\n","    \n","    loss_buffer = []\n","    \n","    for i, (x, y) in enumerate(train_loader):  \n","        \n","        x = x.cuda()\n","        y = y.cuda()\n","        \n","        optimizer.zero_grad()\n","        # get initial hidden state\n","        hidden = rnn.init_hidden(x.shape[0])\n","        \n","        # get output for the sample, remember that we treat it as a sequence\n","        # so you need to iterate over the 2nd, time dimensiotn\n","\n","        seq_len = x.shape[1]\n","        for t in range(seq_len): #wywołyjemy A tyle razy ile mamy liter z imieniu\n","            x_t = x[:, t] #batch_size, input_size - kolejny znak\n","            output, hidden = rnn(x_t, hidden) #ukrytra reprezentacja, kolejne  w kolejnych krokach\n","        \n","        # output = \n","            \n","        loss = cross_entropy(output, y)\n","        loss.backward()\n","        optimizer.step()  \n","        \n","        loss_buffer.append(loss.item())\n","        \n","        if i % 1000 == 1:\n","            print(f\"Epoch: {epoch} Progress: {100 * i/len(train_loader):2.0f}% Loss: {np.mean(loss_buffer):.3f}\")\n","            loss_buffer = []\n","    \n","\n","# evaluate on the test set\n","with torch.no_grad():\n","    ps = []\n","    ys = []\n","    correct = 0\n","    for i, (x, y) in enumerate(test_loader):\n","        x = x.cuda()\n","        ys.append(y.numpy())\n","\n","        hidden = rnn.init_hidden(x.shape[0])\n","        seq_len = x.shape[1]\n","        \n","        for t in range(seq_len): #wywołyjemy A tyle razy ile mamy ;iter z imieniu\n","            x_t = x[:, t] #batch_size, input_size - kolejny znak\n","            output, hidden = rnn(x_t, hidden)\n","       \n","        pred = output.argmax(dim=1)\n","        ps.append(pred.cpu().numpy())\n","    \n","    ps = np.concatenate(ps, axis=0)\n","    ys = np.concatenate(ys, axis=0)\n","    f1 = f1_score(ys, ps, average='weighted')\n","    \n","    print(f\"Final F1 score: {f1:.2f}\")\n","    assert f1 > 0.15, \"You should get over 0.15 f1 score, try changing some hiperparams!\""],"execution_count":10,"outputs":[{"output_type":"stream","text":["Epoch: 0 Progress:  0% Loss: 2.946\n","Epoch: 0 Progress:  6% Loss: 2.865\n","Epoch: 0 Progress: 11% Loss: 2.780\n","Epoch: 0 Progress: 17% Loss: 2.493\n","Epoch: 0 Progress: 22% Loss: 2.239\n","Epoch: 0 Progress: 28% Loss: 2.063\n","Epoch: 0 Progress: 33% Loss: 2.017\n","Epoch: 0 Progress: 39% Loss: 1.946\n","Epoch: 0 Progress: 44% Loss: 1.857\n","Epoch: 0 Progress: 50% Loss: 1.825\n","Epoch: 0 Progress: 55% Loss: 1.818\n","Epoch: 0 Progress: 61% Loss: 1.762\n","Epoch: 0 Progress: 66% Loss: 1.693\n","Epoch: 0 Progress: 72% Loss: 1.659\n","Epoch: 0 Progress: 77% Loss: 1.636\n","Epoch: 0 Progress: 83% Loss: 1.691\n","Epoch: 0 Progress: 89% Loss: 1.609\n","Epoch: 0 Progress: 94% Loss: 1.614\n","Epoch: 0 Progress: 100% Loss: 1.622\n","Final F1 score: 0.18\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sNeNU93qn7BC"},"source":["## Zadanie 2. (0.5 pkt.)\n","Zaimplementuj funkcje `predict`, która przyjmuje nazwisko w postaci stringa oraz model RNN i wypisuje 3 najlepsze predykcje narodowości dla tego nazwiska razem z ich logitami.\n","\n","**Hint**: Przyda się tutaj jedna z funkcji z pierwszej komórki notebooka."]},{"cell_type":"code","metadata":{"id":"N8FhF_08hAQy","executionInfo":{"status":"ok","timestamp":1611177260623,"user_tz":-60,"elapsed":1063,"user":{"displayName":"Jagoda Dziadosz","photoUrl":"","userId":"11816835418749834277"}}},"source":["def predict(name: str, rnn: RNN):\n","    \"\"\"Prints the name and model's top 3 predictions with scores\"\"\"\n","    x = line_to_tensor(name).unsqueeze(0).cuda()\n","    hidden = rnn.init_hidden(x.shape[0])\n","    seq_len = x.shape[1]\n","\n","    for t in range(seq_len): \n","      x_t = x[:, t]\n","      output, hidden = rnn(x_t,hidden)\n","\n","    res = output.topk(3, dim=1)\n","\n","    for score, ind in zip(res.values[0], res.indices[0]):\n","        print(f\"\\t{label_to_idx[ind.item()]}: {score:.2f}\")"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z4OWP8wqhAQy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611177264162,"user_tz":-60,"elapsed":1051,"user":{"displayName":"Jagoda Dziadosz","photoUrl":"","userId":"11816835418749834277"}},"outputId":"bbc36bea-8f82-4bac-cee5-60124df470c6"},"source":["some_names = [\"Satoshi\", \"Jackson\", \"Schmidhuber\", \"Hinton\", \"Kowalski\"]\n","\n","for name in some_names:\n","    print(name)\n","    predict(name, rnn)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Satoshi\n","\tjapanese: 1.99\n","\tarabic: 1.94\n","\tczech: 1.75\n","Jackson\n","\tscottish: 3.28\n","\tenglish: 2.42\n","\tdutch: 1.50\n","Schmidhuber\n","\tgerman: 3.14\n","\tdutch: 2.42\n","\tczech: 2.21\n","Hinton\n","\tscottish: 2.17\n","\tkorean: 2.09\n","\tenglish: 1.93\n","Kowalski\n","\tpolish: 4.87\n","\tczech: 3.16\n","\tjapanese: 2.94\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nNETvP06hAQz"},"source":["## Zadanie 3 (4 pkt.)\n","Ostatnim zadaniem jest implementacji komórki i sieci LSTM. \n","\n","![lstm](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n","\n","* W klasie `LSTMCell` ma znaleźć się główna loginka LSTMa, czyli wszystkie wagi do stanów `hidden` i `cell` jak i bramek kontrolujących te stany. \n","* W klasie `LSTM` powinno znaleźć się wywołanie komórki LSTM, HINT: poprzednio było w pętli uczenia, teraz przenisiemy to do klasy modelu.\n","* W pętli uczenia należy uzupełnić brakujące wywołania do uczenia i ewaluacji modelu.\n","\n","Zdecydowanie polecam [materiały Chrisa Olaha](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) do zarówno zrozumienia jak i ściągi do wzorów.\n","\n","Zadaniem jest osiągnięcie wartości `f1_score` lepszej niż na sieci RNN, przy prawidłowej implementacji nie powinno być z tym problemów używając podanych hiperparametrów. Dozwolona jest oczywiście zmiana `random seed`.\n","\n","#### Komórka LSTM"]},{"cell_type":"code","metadata":{"id":"GNKRxYwChAQz","executionInfo":{"status":"ok","timestamp":1611177494686,"user_tz":-60,"elapsed":1065,"user":{"displayName":"Jagoda Dziadosz","photoUrl":"","userId":"11816835418749834277"}}},"source":["class LSTMCell(torch.nn.Module):\n","\n","    def __init__(self, \n","                 input_size: int, \n","                 hidden_size: int):\n","        \"\"\"\n","        :param input_size: int\n","            Dimensionality of the input vector\n","        :param hidden_size: int\n","            Dimensionality of the hidden space\n","        \"\"\"\n","        \n","        super(LSTMCell, self).__init__()\n","        \n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","\n","        # initialize LSTM weights \n","        # NOTE: there are different approaches that are all correct \n","        # (e.g. single matrix for all input opperations), you can pick\n","        # whichever you like for this task\n","    \n","        self.W_f = torch.nn.Linear(in_features=hidden_size + input_size, out_features=hidden_size)\n","        self.W_i = torch.nn.Linear(in_features=hidden_size + input_size, out_features=hidden_size)\n","        self.W_c = torch.nn.Linear(in_features=hidden_size + input_size, out_features=hidden_size)\n","        self.W_o = torch.nn.Linear(in_features=hidden_size + input_size, out_features=hidden_size)\n","    \n","    def forward(self, \n","                input: torch.tensor, \n","                states: Tuple[torch.tensor, torch.tensor]) -> Tuple[torch.tensor, torch.tensor]:\n","        \n","        hidden, cell = states\n","        \n","        # Compute input, forget, and output gates\n","        # then compute new cell state and hidden state\n","        # see http://colah.github.io/posts/2015-08-Understanding-LSTMs/ \n","\n","\n","        combined = torch.cat([input, hidden], dim=1)\n","        \n","        forget_gate = torch.sigmoid(self.W_f(combined))\n","        input_gate = torch.sigmoid(self.W_i(combined))\n","        cell_update = torch.tanh(self.W_c(combined))\n","        ot = torch.sigmoid(self.W_o(combined))\n","\n","        cell = forget_gate * cell + input_gate * cell_update\n","        \n","        hidden = ot*torch.tanh(cell)\n","        \n","        return hidden, cell"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5U5U8kizhAQz"},"source":["### Klasa modelu LSTM"]},{"cell_type":"code","metadata":{"id":"G2MyIu3_hAQz","executionInfo":{"status":"ok","timestamp":1611177657142,"user_tz":-60,"elapsed":3470,"user":{"displayName":"Jagoda Dziadosz","photoUrl":"","userId":"11816835418749834277"}}},"source":["class LSTM(torch.nn.Module):\n","\n","    def __init__(self, \n","                 input_size: int, \n","                 hidden_size: int):\n","        \"\"\"\n","        :param input_size: int\n","            Dimensionality of the input vector\n","        :param hidden_size: int\n","            Dimensionality of the hidden space\n","        \"\"\"\n","        \n","        super(LSTM, self).__init__()\n","        \n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","\n","        self.cell = LSTMCell(input_size=input_size, hidden_size=hidden_size)\n","        \n","    def forward(self, \n","                input: torch.tensor) -> Tuple[torch.tensor, torch.tensor]:\n","        \"\"\"\n","        :param input: torch.tensor \n","            Input tesnor for a single observation at timestep t\n","            shape [batch_size, input_size]\n","        Returns Tuple of two torch.tensors, both of shape [seq_len, batch_size, hidden_size]\n","        \"\"\"\n","        \n","        batch_size = input.shape[0]\n","        \n","        hidden, cell = self.init_hidden_cell(batch_size)\n","        \n","        hiddens = []\n","        cells = []\n","        \n","        # this time we will process the whole sequence in the forward method\n","        # as oppose to the previous exercise, remember to loop over the timesteps\n","        \n","        time_steps = input.shape[1]\n","\n","        for t in range(time_steps):\n","          x = input[:, t] #[batch_size, input_size]\n","          hidden, cell = self.cell(x,(hidden,cell))\n","          hiddens.append(hidden)\n","          cells.append(cell) \n","\n","        return hiddens, cells\n","    \n","    def init_hidden_cell(self, batch_size):\n","        \"\"\"\n","        Returns initial value for the hidden and cell states\n","        \"\"\"\n","        return (torch.zeros(batch_size, self.hidden_size, requires_grad=True).cuda(), \n","                torch.zeros(batch_size, self.hidden_size, requires_grad=True).cuda())"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3qRxPI-nhAQz"},"source":["### Pętla uczenia"]},{"cell_type":"code","metadata":{"id":"4LVCWqsVhAQ0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611177889048,"user_tz":-60,"elapsed":139135,"user":{"displayName":"Jagoda Dziadosz","photoUrl":"","userId":"11816835418749834277"}},"outputId":"cbd992a5-82cb-4293-a2ed-04e2ee803328"},"source":["from itertools import chain\n","\n","# torch.manual_seed(1337)\n","\n","# build data loaders\n","train_loader = DataLoader(train_dataset, batch_size=1, sampler=sampler)\n","test_loader = DataLoader(test_dataset, batch_size=1)\n","\n","# initialize the lstm with an additional cliassifier layer at the top\n","lstm = LSTM(input_size=len(all_letters), hidden_size=128).cuda()\n","clf = torch.nn.Linear(in_features=128, out_features=len(label_to_idx)).cuda()\n","\n","# initialize a optimizer\n","params = chain(lstm.parameters(), clf.parameters())\n","optimizer = torch.optim.Adam(params, lr=0.01) \n","\n","# we will train for only a single epoch \n","epoch = 1\n","\n","# main loop\n","for epoch in range(epoch):\n","    \n","    loss_buffer = []\n","    \n","    for i, (x, y) in enumerate(train_loader):   \n","        \n","        x = x.cuda()\n","        y = y.cuda()\n","        \n","        optimizer.zero_grad()\n","        \n","        # get output for the sample, remember that we treat it as a sequence\n","        # so you need to iterate over the sequence length here\n","        # don't forget about the classifier!\n","        \n","        hidden, state = lstm(x)\n","        output = clf(hidden[-1])\n","\n","        # calucate the loss\n","        loss = cross_entropy(output, y)\n","        loss.backward()\n","        optimizer.step()                                \n","        \n","        loss_buffer.append(loss.item())\n","        \n","        if i % 1000 == 1:\n","            print(f\"Epoch: {epoch} Progress: {100 * i/len(train_loader):2.0f}% Loss: {np.mean(loss_buffer):.3f}\")\n","            loss_buffer = []\n","\n","# evaluate on the test set\n","with torch.no_grad():\n","    \n","    ps = []\n","    ys = []\n","    for i, (x, y) in enumerate(test_loader): \n","        \n","        x = x.cuda()\n","        ys.append(y.numpy())\n","        \n","        hidden, state = lstm(x)\n","        output = clf(hidden[-1])\n","\n","        pred = output.argmax(dim=1)\n","        ps.append(pred.cpu().numpy())\n","    \n","    ps = np.concatenate(ps, axis=0)\n","    ys = np.concatenate(ys, axis=0)\n","    f1 = f1_score(ys, ps, average='weighted')\n","    \n","    print(f\"Final F1 score: {f1:.2f}\")\n","    assert f1 > 0.18, \"You should get over 0.18 f1 score, try changing some hiperparams!\""],"execution_count":18,"outputs":[{"output_type":"stream","text":["Epoch: 0 Progress:  0% Loss: 2.866\n","Epoch: 0 Progress:  6% Loss: 2.458\n","Epoch: 0 Progress: 11% Loss: 1.874\n","Epoch: 0 Progress: 17% Loss: 1.654\n","Epoch: 0 Progress: 22% Loss: 1.613\n","Epoch: 0 Progress: 28% Loss: 1.370\n","Epoch: 0 Progress: 33% Loss: 1.339\n","Epoch: 0 Progress: 39% Loss: 1.297\n","Epoch: 0 Progress: 44% Loss: 1.248\n","Epoch: 0 Progress: 50% Loss: 1.223\n","Epoch: 0 Progress: 55% Loss: 1.111\n","Epoch: 0 Progress: 61% Loss: 1.095\n","Epoch: 0 Progress: 66% Loss: 1.086\n","Epoch: 0 Progress: 72% Loss: 1.011\n","Epoch: 0 Progress: 77% Loss: 1.051\n","Epoch: 0 Progress: 83% Loss: 0.978\n","Epoch: 0 Progress: 89% Loss: 0.950\n","Epoch: 0 Progress: 94% Loss: 0.957\n","Epoch: 0 Progress: 100% Loss: 0.877\n","Final F1 score: 0.22\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gGXUhgroo7AN"},"source":["## Zadanie 4. (0.5 pkt.)\n","Zaimplementuj analogiczną do funkcji `predict` z zadania 2 dla modelu `lstm+clf`.\n"]},{"cell_type":"code","metadata":{"id":"-ChJv1fphAQ0","executionInfo":{"status":"ok","timestamp":1611178028158,"user_tz":-60,"elapsed":548,"user":{"displayName":"Jagoda Dziadosz","photoUrl":"","userId":"11816835418749834277"}}},"source":["def predict_lstm(name: str, lstm: LSTM, clf: torch.nn.Module):\n","    \"\"\"Prints the name and model's top 3 predictions with scores\"\"\"\n","    \n","    x = line_to_tensor(name).unsqueeze(0).cuda()\n","    hidden = lstm.init_hidden_cell(x.shape[0])\n","    seq_len = x.shape[1]\n","\n","    for t in range(seq_len):\n","      # x_t = x[:, t]\n","      hidden, state = lstm(x)\n","      output = clf(hidden[-1])\n","    \n","    \n","    res = output.topk(3, dim=1)\n"," \n","    for score, ind in zip(res.values[0], res.indices[0]):\n","        print(f\"\\t{label_to_idx[ind.item()]}: {score:.2f}\")"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"pgQcGWqthAQ0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611178031095,"user_tz":-60,"elapsed":558,"user":{"displayName":"Jagoda Dziadosz","photoUrl":"","userId":"11816835418749834277"}},"outputId":"4ba7d0e5-b706-4292-a634-36d402f2fd12"},"source":["# test your lstm predictor\n","some_names = [\"Satoshi\", \"Jackson\", \"Schmidhuber\", \"Hinton\", \"Kowalski\"]\n","    \n","for name in some_names:\n","    print(name)\n","    predict_lstm(name, lstm, clf)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Satoshi\n","\tjapanese: 6.80\n","\tarabic: 2.05\n","\trussian: 1.65\n","Jackson\n","\tscottish: 4.69\n","\trussian: 2.99\n","\tenglish: 0.29\n","Schmidhuber\n","\tczech: 3.86\n","\tdutch: 2.45\n","\tgerman: 2.11\n","Hinton\n","\tdutch: 2.26\n","\tscottish: 1.46\n","\tfrench: 0.83\n","Kowalski\n","\tpolish: 7.95\n","\trussian: 1.68\n","\tczech: 1.49\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PpQ580-dsbVd"},"source":[""],"execution_count":null,"outputs":[]}]}